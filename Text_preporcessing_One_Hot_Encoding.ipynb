{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrQrV02kr5R95OHuhLF6Xi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rlackdals981010/NLP_STUDY_WITH_DL/blob/main/Text_preporcessing_One_Hot_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8Cm5sxDuFsb-"
      },
      "outputs": [],
      "source": [
        "#정수 인코딩중 가장 기본적인 표현 방법. 반드시 알아야함\n",
        "#단어 집합 : 서로 다른 단어들의 집합. book과 books도 다른 단어임.\n",
        "#단어를 숫자(벡터)로 바꾸는 원-핫 인코딩에 대해서 알아보자"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#원-핫 인코딩이란?\n",
        "#단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1을 부여하고, 다른 인덱스에는 0을 부여하는\n",
        "#단어의 표현 방법. -> 결과는 원-핫 벡터\n",
        "\n",
        "#과정 1. 정수 인코딩 수행\n",
        "#과정 2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주, 해당 위치에 1을 부여하고, 다른 단어에 위치에는 0"
      ],
      "metadata": {
        "id": "4p1g6vZ0GENH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt=Okt()\n",
        "tokens =okt.morphs(\"나는 자연어 처리를 배운다\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bvg6YvQHC9X",
        "outputId": "995bb0d7-40b0-4181-f7d9-8d987644b2e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n",
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
        "print('단어 집합 :',word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIl_-A5OHLb4",
        "outputId": "0f7ba799-882a-4fd3-9163-e1c9a6e36909"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(word,word_to_index):\n",
        "  one_hot_vector=[0]*(len(word_to_index))\n",
        "  index = word_to_index[word]\n",
        "  one_hot_vector[index]=1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "L49yijTdHekb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding(\"자연어\",word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrxynLLMHmrU",
        "outputId": "abaaba11-f650-4c1b-c3ab-1bfa735e1bc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print('단어 집합:',tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DhQ8LkzH46d",
        "outputId": "ee1ccdbf-fd7e-4504-f078-87e927b3ce4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#위 단어 집합으로만 만들어진 텍스트로 확인\n",
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0uvWXRNI6q_",
        "outputId": "83b9818e-d254-47ab-e894-3d54b6943f60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 1, 6, 3, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#위 결과는 정수 인코딩이고 다음부터 원-핫 인코딩 진행\n",
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)\n",
        "#[2, 5, 1, 6, 3, 7] 이게 다음 인덱스 0~7로 표현된걸 볼 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmCC3fNRTn1M",
        "outputId": "82c49a5e-07f2-4a49-bacd-b0bd28d40ca1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문제 1. 근데 원핫 벡터를 보면 단어의 종류가 많아질수록 벡터의 길이도 무한대로 늘어나기 때무넹 공간이 버거워짐\n",
        "#문제 2. 단어의 유사도 표현도 불가함. 늑대-강아지, 호랑이-고양이가 유사함을 사람은 아는데 원-핫 벡터는 [1. 0. 0. 0.]이런식이니까 모름\n",
        "#이를 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 2개가 있음\n",
        "#방법 1. Count 기반의 벡터화인 LSA(잠재 의미 분석), HAL\n",
        "#방법 2. 예측 기반으로 벡터화인 NNLM, RNNLM, Word2Vec, FastText\n",
        "#방법 1+2. 카운트+예측인 GloVe\n",
        "#이것들은 CH.9인 워드 임베딩"
      ],
      "metadata": {
        "id": "Wmh3m6HnUGhw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THo6H9GjVafq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}